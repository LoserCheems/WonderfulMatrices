# Logging and Output arguments
logging_dir: ./logs
logging_steps: 100
report_to:
- tensorboard
- wandb
output_dir: ./results

# Model arguments
model_config:
  vocab_size: 32768
  hidden_size: 256
  intermediate_size: 512
  num_hidden_layers: 8
  hidden_dropout: 0.0
  hidden_act: "silu"
  max_position_embeddings: 2048
  rope_theta: 10000.0
  use_cache: True
  tie_word_embeddings: True
  num_attention_heads: 2
  num_key_value_heads: 1
  is_moe: True
  num_cdmmoe_experts: 512
  num_cdmmoe_heads: 1
  num_cdmmoe_experts_per_head: 2
  private_expert_retrieval_size: 256

tokenizer_name_or_path: JingzeShi/Doge-tokenizer
torch_dtype: bfloat16
resume_from_checkpoint: null

# Dataset arguments
dataset_path: ./datasets/pretrain_dataset
dataset_splits:
- train
- test
mlm:
  mlm: False
  mlm_probability: 0.0

# Trainer arguments
seed: 233

do_train: True
max_train_steps: 8000
per_device_train_batch_size: 1

do_eval: True
eval_strategy: steps
eval_steps: 100
per_device_eval_batch_size: 1

optim: adamw_torch_fused
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-8
learning_rate: 8.0e-3
lr_scheduler_type: warmup_stable_decay
lr_scheduler_kwargs:
  warmup_type: linear
  decay_type: linear
  num_decay_steps: 800
  min_lr_ratio: 0.0
warmup_steps: 800
weight_decay: 0.01
gradient_accumulation_steps: 256
max_grad_norm: 1.0
bf16: True

save_safetensors: True
save_strategy: steps
save_steps: 1000

push_to_hub: True
repo_id: <your-repo-id>
private: True
token: <your-token>
